{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 2: Exploring Image Data\n",
    "### Group Members: Andrew Breslauer, Camilo Villamizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Business Understanding\n",
    "This dataset is a total of over 372,000 images of 28x28 handwritten English letters. This data was initially selected in the hopes of machine-reading human handwriting, which is very different from machine-reading machine-printed letters, so read older transcribed texts. The eventual goal of this particular model is the same, hoping to be able to recognize various handwritten letters in the hopes of transcribing older texts into digital text where it can be more easily preserved. Some interested parties may include various museums or religious sects looking to preserve religious texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following block of code is not mine. \n",
    "This code block was provided by the dataset creator to convert between his provided CSV file and PNG files. All that was provided by the creator other than this was a CSV file where every column was an RGB value for a pixel of the image, and every row was a new image. This is the eventual data we want in the numpy arrays, but converting back to images first allows us to both confirm that the produced images match the original images, as well as start the lab from the correct starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prcessing Alphabet - A\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n",
      "Images processed: 10000\n",
      "Images processed: 11000\n",
      "Images processed: 12000\n",
      "Images processed: 13000\n",
      "\n",
      "Prcessing Alphabet - B\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "\n",
      "Prcessing Alphabet - C\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n",
      "Images processed: 10000\n",
      "Images processed: 11000\n",
      "Images processed: 12000\n",
      "Images processed: 13000\n",
      "Images processed: 14000\n",
      "Images processed: 15000\n",
      "Images processed: 16000\n",
      "Images processed: 17000\n",
      "Images processed: 18000\n",
      "Images processed: 19000\n",
      "Images processed: 20000\n",
      "Images processed: 21000\n",
      "Images processed: 22000\n",
      "Images processed: 23000\n",
      "\n",
      "Prcessing Alphabet - D\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n",
      "Images processed: 10000\n",
      "\n",
      "Prcessing Alphabet - E\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n",
      "Images processed: 10000\n",
      "Images processed: 11000\n",
      "\n",
      "Prcessing Alphabet - F\n",
      "Images processed: 1000\n",
      "\n",
      "Prcessing Alphabet - G\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "\n",
      "Prcessing Alphabet - H\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "\n",
      "Prcessing Alphabet - I\n",
      "Images processed: 1000\n",
      "\n",
      "Prcessing Alphabet - J\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "\n",
      "Prcessing Alphabet - K\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "\n",
      "Prcessing Alphabet - L\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n",
      "Images processed: 10000\n",
      "Images processed: 11000\n",
      "\n",
      "Prcessing Alphabet - M\n",
      "Images processed: 1000\n",
      "Images processed: 2000\n",
      "Images processed: 3000\n",
      "Images processed: 4000\n",
      "Images processed: 5000\n",
      "Images processed: 6000\n",
      "Images processed: 7000\n",
      "Images processed: 8000\n",
      "Images processed: 9000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This script was included with the dataset to convert the included .csv file into images\n",
    "# Dataset: https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format\n",
    "# Script: https://www.kaggle.com/sachinpatel21/csv-to-images\n",
    "# Warning: This will take a while, don't run unless refreshing image set\n",
    "import csv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "# Comment everything from here to the end after running once\n",
    "# Took 13min 10s on my machine\n",
    "csv_File_Path = \"./Letters/A_Z Handwritten Data.csv\"\n",
    "\n",
    "count = 1\n",
    "last_digit_Name =  None\n",
    "\n",
    "image_Folder_Path = \"./Letter_Images\"\n",
    "\n",
    "Alphabet_Mapping_List = list(string.ascii_uppercase)\n",
    "\n",
    "for alphabet in Alphabet_Mapping_List:\n",
    "    path = image_Folder_Path + '\\\\' + alphabet\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "with open(csv_File_Path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        digit_Name = row.pop(0)\n",
    "        image_array = np.asarray(row)\n",
    "        image_array = image_array.reshape(28, 28)\n",
    "        new_image = Image.fromarray(image_array.astype('uint8'))\n",
    "\n",
    "        if last_digit_Name != str(Alphabet_Mapping_List[(int)(digit_Name)]):\n",
    "            last_digit_Name = str(Alphabet_Mapping_List[(int)(digit_Name)])\n",
    "            count = 0\n",
    "            print (\"\")\n",
    "            print (\"Prcessing Alphabet - \" + str (last_digit_Name))\n",
    "        \n",
    "        image_Path = image_Folder_Path + '\\\\' + last_digit_Name + '\\\\' + str(last_digit_Name) + '-' + str(count) + '.png'\n",
    "        new_image.save(image_Path)\n",
    "        count = count + 1\n",
    "\n",
    "        if count % 1000 == 0:\n",
    "            print (\"Images processed: \" + str(count))\n",
    "\n",
    "# print(\"Uncomment to run again (will take some time).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Read in as numpy arrays and 2.2: Linearize images to create a table of 1-D image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import imageio\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color\n",
    "\n",
    "images = []\n",
    "letters = []\n",
    "letter_change_indices = []\n",
    "\n",
    "counter = 0\n",
    "total_counter = 0\n",
    "letter_limit = 1000   # Remove for final run\n",
    "for letter in enumerate(string.ascii_uppercase):\n",
    "    counter = 0\n",
    "    letter_change_indices.append(total_counter)\n",
    "    print(\"Processing\",letter[1])\n",
    "    path_folder = './Letter_Images/' + letter[1]\n",
    "    for path_image in os.listdir(path_folder):\n",
    "        path = path_folder + '/' + path_image\n",
    "        counter+=1\n",
    "        total_counter+=1\n",
    "        if counter >= letter_limit: \n",
    "             # Limit to 1000 images per letter to prevent bloating the data. Remove once all data established.\n",
    "            break\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = color.rgb2gray(img) # technically \n",
    "        img = np.ravel(img)\n",
    "        images.append(img)\n",
    "        letters.append(letter)\n",
    "print(\"Total Images Processed:\",total_counter)\n",
    "X = np.array(images)\n",
    "Y = np.array(letters)\n",
    "h, x = (28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Visualizing Images\n",
    "Print one of each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in letter_change_indices:\n",
    "    img = X[i]\n",
    "    reshaped_img = np.reshape(img, (h,x))\n",
    "    plt.imshow(Image.fromarray(reshaped_img))\n",
    "    plt.title(Y[int(i)][1])\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Linear Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Bar, Line\n",
    "    from plotly.graph_objs import Scatter, Layout\n",
    "    from plotly.graph_objs.scatter import Marker\n",
    "    from plotly.graph_objs.layout import XAxis, YAxis\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "    \n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "    \n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, x, n_row=4, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        reshaped_img = images[i].reshape((h,x))\n",
    "        plt.imshow((reshaped_img), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 300\n",
    "print (\"Extracting top %d components from %d faces\" % (n_components, total_counter))\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(images.copy())\n",
    "eigenimages = pca.components_.reshape((n_components, h, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What types of accuracy can we have?</h4>\n",
    "<table style=float:left>\n",
    "  <tr>\n",
    "    <th>Explained Variance</th>\n",
    "    <th># of Components</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>70%</td>\n",
    "    <td>24</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>80%</td>\n",
    "    <td>38</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>90%</td>\n",
    "    <td>70</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>95%</td>\n",
    "    <td>114</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p style=float:left>\n",
    "For our data, many letters can look similar to one another, such as capital G and captial C, so a higher accuracy is probably required. Due to this, we will be using the 90% explained variance, or the 70 most important components.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eigenletter_titles = [\"eigenletter %d\" % i for i in range(eigenimages.shape[0])]\n",
    "plot_gallery(eigenimages, eigenletter_titles, h, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### They're all barely recognizable!\n",
    "Yes they are! Let's try reconstructing them and seeing how they turn out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(trans_obj,org_features):\n",
    "    low_rep = trans_obj.transform(org_features)\n",
    "    rec_image = trans_obj.inverse_transform(low_rep)\n",
    "    return low_rep, rec_image\n",
    "    \n",
    "idx_to_reconstruct = 0 \n",
    "X_idx = X[idx_to_reconstruct]\n",
    "low_dimensional_representation, reconstructed_image = reconstruct_image(pca,X_idx.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X_idx.reshape((h, x)), cmap=plt.cm.gray)\n",
    "plt.title('Original')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.grid(False)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_image.reshape((h, x)), cmap=plt.cm.gray)\n",
    "plt.title('Reconstructed from Full PCA')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isn't she beautiful!\n",
    "Hopefully you can see the letter A in the reconstructed image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import KernelPCA\n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenfaces from %d faces, and calculating inverse transform\" % (n_components, X.shape[0]))\n",
    "\n",
    "kpca = KernelPCA(n_components=n_components, kernel='rbf', \n",
    "                fit_inverse_transform=True, gamma=12.2, # very sensitive to the gamma parameter,\n",
    "                remove_zero_eig=True)  \n",
    "kpca.fit(X.copy())\n",
    "\n",
    "# This will take a long time. Do not be afraid.\n",
    "# Took 21min 33s on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(kpca, open('large_data/kpca.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = pickle.load(open('large_data/kpca.p', 'rb'))\n",
    "from ipywidgets import widgets\n",
    "import warnings\n",
    "# warnings.simplefilter('ignore', DeprecationWarning)\n",
    "# warnings.simplefilter(\"always\",DeprecationWarning)\n",
    "\n",
    "\n",
    "\n",
    "def plt_reconstruct(idx_to_reconstruct):\n",
    "    idx_to_reconstruct = np.round(idx_to_reconstruct)\n",
    "    \n",
    "    reconstructed_image = pca.inverse_transform(pca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "    reconstructed_image_kpca = kpca.inverse_transform(kpca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(X[idx_to_reconstruct].reshape((h,x)), cmap=plt.cm.gray)\n",
    "    plt.title(Y[idx_to_reconstruct][1])\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(reconstructed_image.reshape((h,x)), cmap=plt.cm.gray)\n",
    "    plt.title('Full PCA')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(reconstructed_image_kpca.reshape((h,x)), cmap=plt.cm.gray)\n",
    "    plt.title('Kernel PCA')\n",
    "    plt.grid()\n",
    "    \n",
    "    \n",
    "widgets.interact(plt_reconstruct,idx_to_reconstruct=(0,total_counter-1,1),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well that's not good!\n",
    "The kPCA, mostly, doesn't resolve back to a recognizeable version of the letter. However, there are some examples (see the letter G, id 6978) where the kPCA image does almost resemble the original letter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Compare Linear (PCA) vs Non-Linear (kPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Feature Extraction\n",
    "<b>First, gradients</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gradients\n",
    "from skimage.io import imshow\n",
    "\n",
    "idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "img  = X[idx_to_reconstruct].reshape((h,x))\n",
    "imshow(img)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import sobel_h, sobel_v\n",
    "from skimage import io, color\n",
    "img_grey = color.rgb2gray(img)\n",
    "\n",
    "gradient_mag = np.sqrt(sobel_v(img_grey)**2 + sobel_h(img_grey)**2 ) \n",
    "imshow(gradient_mag)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Next, DAISY Bag of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape),step=10, radius=10, rings=2, histograms=6, orientations=8, visualize=False)\n",
    "    return feat.reshape((-1))\n",
    "\n",
    "%time \n",
    "test_feature = apply_daisy(X[3],(h,x))\n",
    "print(test_feature.shape)\n",
    "\n",
    "%time \n",
    "\n",
    "daisy_features = np.apply_along_axis(apply_daisy, 1, X, (h,x))\n",
    "print(daisy_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix = pairwise_distances(daisy_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import fixed, widgets\n",
    "def closest_image(dmat,idx1):\n",
    "    distances = copy.deepcopy(dmat[idx1,:])\n",
    "    distances[idx1] = np.infty\n",
    "    idx2 = np.argmin(distances)\n",
    "    \n",
    "    distances[idx2] = np.infty\n",
    "    idx3 = np.argmin(distances)\n",
    "    \n",
    "    plt.figure(figsize=(10,16))\n",
    "    plt.subplot(1,3,1)\n",
    "    imshow(X[idx1].reshape((h,x)))\n",
    "    plt.title(\"Original Image \"+Y[idx1][1])\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    imshow(X[idx2].reshape((h,x)))\n",
    "    plt.title(\"Closest Image  \"+Y[idx1][1])\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    imshow(X[idx3].reshape((h,x)))\n",
    "    plt.title(\"Next Closest Image \"+Y[idx1][1])\n",
    "    plt.grid()\n",
    "    \n",
    "widgets.interact(closest_image,idx1=(0,total_counter-26,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Does the Feature Extraction Show Promise?\n",
    "The gradient/DAISY Bag of Features method does show promise for this dataset because each letter has similarly defined gradient lines, because that is all that the letter is. Some sloppily drawn Cs may be mistaken for Gs and vice versa, for example, but for the most part letters that are cleanly drawn follow the same basic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
